[{"title": "3. Prozesse - Lernportal Betriebssysteme", "href": "03_prozesse.html", "text": "Kapitel 3: Prozesse Das zentrale Konzept zur Verwaltung von Programmausführungen. Was ist ein Prozess? Ein Prozess ist eines der fundamentalsten Konzepte eines Betriebssystems. Er repräsentiert die  Zu einem Prozess gehören alle Ressourcen, die zur Ausführung benötigt werden: Beispiel: Ein Programm als Prozess Betrachten wir ein einfaches Programm, das eine Schleife ausführt:  Während der Ausführung ändert sich der  Prozess speichern und reaktivieren Ein Prozess lässt sich an einem beliebigen Punkt unterbrechen, sein Zustand (PC, Register, Speicherzuweisungen) wird im PCB gesichert und kann später exakt an dieser Stelle fortgesetzt werden. Diese Fähigkeit bildet die Grundlage für Multitasking und Kontextwechsel. Viele Prozesse auf einem Rechner Zu jedem Zeitpunkt existieren auf einem Rechner viele Prozesse: Das Betriebssystem ist verantwortlich für die  Der Prozesskontrollblock (Process Control Block, PCB) Um die vielen laufenden Prozesse verwalten zu können, legt das Betriebssystem für jeden einzelnen Prozess einen  Typische Inhalte eines PCB sind: Prozesszustände und Zustandsübergänge Ein kommunizierender Prozess wechselt zwischen Aktiv, Blockiert und Bereit, wenn er Nachrichten sendet/empfängt: Verdrängung sorgt für gerechte CPU-Verteilung (Fairness) und schnelle Reaktionszeiten, da keine Prozesse die CPU monopolisiert. Kontextwechsel (Context Switch) Beispiel: Programmzähler-Abfolge bei 3 Prozessen Beispiel: CPU-Zeitscheiben-Diagramm Beispiel: Task Manager Prozesserzeugung Aufteilung einer Aufgabe in eine Kette von Teilprozessen (Producer-Consumer): Vergleich des Durchsatzes ohne und mit Pipelining anhand von I/O- und Rechenzeiten. Erweiterte Prozesshierarchie (Flugsicherung) Komplexes System mit mehreren spezialisierten Prozessen: Prozessterminierung Ein Prozess endet durch: Welcher Zustand folgt nach   Erstellt zur Unterstützung der Prüfungsvorbereitung. Viel Erfolg bei der Klausur!"}, {"title": "5. CPU-Scheduling - Lernportal Betriebssysteme", "href": "05_scheduling.html", "text": "Kapitel 5: CPU-Scheduling Die Kunst, zu entscheiden, welcher  Die Rolle und Ziele des Schedulers Der  Kriterien und Zielkonflikte Ein Scheduler muss verschiedene, teils widersprüchliche Ziele ausbalancieren: Leistungskennzahlen Grundlegende Scheduling-Algorithmen First-Come, First-Served (FCFS) Shortest-Job-First (SJF) / Shortest-Remaining-Time (SRT) Prioritätsbasiertes Scheduling Round Robin (RR) Multilevel Queue Scheduling Mehrere separate Ready-Queues für verschiedene Prozessklassen (z.B. interaktive Vordergrundprozesse, Batch-Hintergrundprozesse). Jede Queue kann einen eigenen Scheduling-Algorithmus verwenden. Multilevel Feedback Queue (MLFQ) Wie Multilevel Queue, aber Prozesse können zwischen den Queues wechseln. Ein Prozess, der viel CPU-Zeit verbraucht, kann in eine niedrigere Prioritäts-Queue mit längerem Quantum verschoben werden. Dies verhindert Verhungern und passt sich dem Prozessverhalten an. Echtzeit-Scheduling Dynamisches, präemptives Verfahren, das dem Prozess mit der frühesten Deadline die höchste Priorität gibt. Es ist optimal auf einem Prozessor, d.h., wenn eine Abfolge existiert, die alle Deadlines einhält, wird EDF sie finden. Unter Überlast kann es jedoch zu einem Dominoeffekt kommen, bei dem viele Tasks ihre Deadlines verpassen. Statisches, präemptives Verfahren für periodische Tasks. Die Priorität wird fest basierend auf der Periodendauer vergeben: je kürzer die Periode, desto höher die Priorität. RMS ist optimal, solange die CPU-Auslastung eine bestimmte Schranke nicht überschreitet: $\\sum (C_i / P_i) \\leq n(2^{1/n}-1)$, wobei C die Ausführungszeit und P die Periode ist. Für viele Prozesse konvergiert diese Schranke gegen $\\ln(2) \\approx 69.3\\%$. Scheduling in Mehrprozessorumgebungen Scheduling in der Praxis Windows Linux Zusammenfassung Welcher Algorithmus verwendet eine feste Zeitscheibe?  Erstellt zur Unterstützung der Prüfungsvorbereitung. Viel Erfolg bei der Klausur!"}, {"title": "6. Synchronisation - Lernportal Betriebssysteme", "href": "06_synchronisation.html", "text": "Kapitel 6: Synchronisation Die Beherrschung des kontrollierten Chaos von nebenläufigen Prozessen und Threads. Das Problem der Nebenläufigkeit Betriebssysteme ermöglichen die  Problematisch, wenn: Wenn mehrere Threads oder Prozesse auf dieselben gemeinsam genutzten Daten zugreifen und das Endergebnis von der nicht vorhersagbaren Reihenfolge der Zugriffe abhängt, spricht man von einer Race Condition. Dies kann zu inkonsistenten Datenzuständen führen. Kritischer Abschnitt und Wechselseitiger Ausschluss Der Teil des Programmcodes, der auf gemeinsame Ressourcen zugreift, wird als  Lösungsansätze Die einfachste Methode auf einem Einprozessorsystem ist das Sperren aller Interrupts vor dem Betreten eines kritischen Abschnitts. Dies verhindert Kontextwechsel und damit den Zugriff durch andere Prozesse. Nachteile sind die Blockade wichtiger E/A-Operationen und die Unwirksamkeit auf Mehrprozessorsystemen. Hierbei wartet ein Prozess in einer aktiven Schleife (Busy Waiting), bis er den kritischen Abschnitt betreten darf. Dies verschwendet CPU-Zyklen. Beispiele sind eine einfache Lock-Variable (die jedoch selbst anfällig für Race Conditions ist) oder der komplexere, aber korrekte Peterson-Algorithmus. Moderne Prozessoren bieten atomare Befehle wie  Semaphore Ein von Dijkstra 1965 eingeführtes Synchronisationsinstrument. Ein Semaphor ist im Wesentlichen eine Zählvariable, auf die nur über zwei atomare Operationen zugegriffen werden kann:   Monitore Ein Monitor ist ein höheres, objektorientiertes Synchronisationskonzept. Er kapselt Daten zusammen mit den Methoden, die darauf operieren. Der entscheidende Punkt ist, dass der Compiler automatisch dafür sorgt, dass sich zu jedem Zeitpunkt nur ein Thread innerhalb einer der Monitormethoden aufhalten kann. Die Methoden sind also implizit durch einen Mutex geschützt. In Java kann jede Klasse als Monitor fungieren. Das Schlüsselwort  Klassische Probleme Ein oder mehrere Erzeuger-Threads produzieren Daten und legen sie in einem begrenzten Puffer ab. Ein oder mehrere Verbraucher-Threads entnehmen die Daten aus dem Puffer. Die Synchronisation muss sicherstellen, dass Erzeuger nicht in einen vollen und Verbraucher nicht aus einem leeren Puffer zugreifen, und dass der Zugriff auf den Puffer selbst wechselseitig ausgeschlossen ist. Mehrere Threads greifen auf eine gemeinsame Datenstruktur zu. Dabei dürfen beliebig viele Leser gleichzeitig zugreifen. Sobald aber ein Schreiber zugreifen will, muss er exklusiven Zugriff erhalten; keine anderen Leser oder Schreiber dürfen aktiv sein. Eine Gruppe von N Threads muss an einem bestimmten Punkt im Code (der Barriere) aufeinander warten. Keiner der Threads darf fortfahren, bevor nicht alle N Threads die Barriere erreicht haben. Dies kann mit einem Zähler, einem Mutex-Semaphor zum Schutz des Zählers und einem zweiten Semaphor als eigentliche Barriere implementiert werden. Generelle Semaphore Synchronisationsdienste in UNIX/Linux Synchronisationsdienste in Windows Welches Problem verhindert ein Mutex?  Erstellt zur Unterstützung der Prüfungsvorbereitung. Viel Erfolg bei der Klausur!"}, {"title": "1. Einführung & Überblick - Lernportal Betriebssysteme", "href": "01_einfuehrung.html", "text": "Kapitel 1: Einführung & Überblick Die Grundlagen und die Rolle von Betriebssystemen in der Informatik. Was ist ein Betriebssystem? Ein Betriebssystem ist die grundlegende Software, die einen Computer erst funktionsfähig macht. Es agiert als zentrale Schnittstelle zwischen der komplexen Hardware des Rechners und der Anwendungssoftware, die der Benutzer ausführt. Man kann es sich als eine Vermittlungsschicht vorstellen, die die Ressourcen des Computers verwaltet und den Programmen zur Verfügung stellt. Im Schichtenmodell eines Computersystems bildet die  Die zentralen Aufgaben eines Betriebssystems Die Hauptaufgaben lassen sich in mehrere Kernbereiche unterteilen: Arten von Betriebssystemen Betriebssysteme lassen sich nach ihrem Haupteinsatzzweck klassifizieren: Struktur eines Betriebssystems Ein Betriebssystem besteht typischerweise aus folgenden Komponenten: Kernel-Architekturen Systemaufrufe Systemaufrufe ermöglichen Programmen, Betriebssystemdienste anzufordern, z.B. für Dateien, Prozesse oder E/A. Typischerweise erfolgt der Wechsel vom User- in den Kernel-Modus über eine Trap-Instruction. Historische Entwicklung Weiterführende Links Wozu dient ein Betriebssystem?  Erstellt zur Unterstützung der Prüfungsvorbereitung. Viel Erfolg bei der Klausur!"}, {"title": "2. Grundlagen - Lernportal Betriebssysteme", "href": "02_grundlagen.html", "text": "Kapitel 2: Grundlagen der Rechnerarchitektur Ein tiefer Einblick in die Hardware-Komponenten, die die Basis für jedes Betriebssystem bilden. Die Von-Neumann-Architektur Moderne Computer basieren fast ausnahmslos auf der Von-Neumann-Architektur. Ihr zentrales Merkmal ist, dass sowohl die auszuführenden  Die Hauptkomponenten sind: Der Befehlszyklus: Wie ein Programm ausgeführt wird Die CPU arbeitet kontinuierlich einen Zyklus ab, um Programme auszuführen. Dieser wird oft als  Nach jedem Befehl wird der Programmzähler (PC) erhöht, damit er auf den nächsten Befehl zeigt. Bei Sprungbefehlen wird der PC auf eine neue Adresse gesetzt. Wichtige CPU-Register im Detail Register sind für die Funktion der CPU unerlässlich. Die wichtigsten Typen sind: Beispiel für verzweigende Rechenausführung Ein Beispiel für eine Schleife, die den Wert von  Der Rechenzustand & Interrupts Der vollständige Zustand eines laufenden Programms zu einem bestimmten Zeitpunkt lässt sich (vereinfacht) beschreiben durch die Summe der Inhalte aller relevanten Speicherbereiche: Ein Interrupt ist ein Mechanismus, der die normale, sequentielle Abarbeitung von Befehlen unterbricht, um auf ein dringendes Ereignis zu reagieren. Wenn ein Interrupt auftritt, sichert die CPU ihren aktuellen Zustand (mindestens den PC und das PSW), springt zu einer vordefinierten Adresse und führt dort eine spezielle  Arten von Interrupts: Die Speicherhierarchie Da schneller Speicher (wie Register) teuer und klein ist, während günstiger Speicher (wie Festplatten) langsam ist, organisieren Computer ihren Speicher in einer Hierarchie. Ziel ist es, die durchschnittliche Zugriffszeit zu minimieren, indem häufig genutzte Daten in den schnelleren Ebenen gehalten werden (Caching). Die typischen Ebenen sind: Vom Quellcode zum Programm Ein in einer Hochsprache (z.B. C, Java) geschriebenes Programm muss mehrere Schritte durchlaufen, bevor es von der CPU ausgeführt werden kann: Rolle von Compiler und Linker Zusammenfassung Wie nennt man einen laufenden Programmkontainer?  Erstellt zur Unterstützung der Prüfungsvorbereitung. Viel Erfolg bei der Klausur!"}, {"title": "9. Speicherverwaltung - Lernportal Betriebssysteme", "href": "09_speicherverwaltung.html", "text": "Kapitel 9: Speicherverwaltung Die Organisation des Hauptspeichers und das Konzept des virtuellen Speichers. Grundlagen und Ziele der Speicherverwaltung Die Speicherverwaltung ist eine der zentralen Aufgaben eines Betriebssystems. Sie wird in enger Zusammenarbeit zwischen der Betriebssystem-Software und spezialisierter Hardware, der  Ihre Hauptziele sind: Logische vs. Physische Adressräume Um diese Ziele zu erreichen, unterscheidet das System strikt zwischen zwei Arten von Adressen: Die Aufgabe der MMU ist es, zur Laufzeit jede logische Adresse, die ein Prozess generiert, in eine physische Adresse zu übersetzen. Paging: Seitenbasierte Speicherverwaltung Paging ist die heute dominierende Methode zur Speicherverwaltung. Die Grundidee ist, das Problem der externen Fragmentierung (viele kleine, ungenutzte Speicherlücken) zu lösen, indem der Speicher nicht mehr in großen, zusammenhängenden Blöcken, sondern in kleinen Einheiten fester Größe verwaltet wird. Eine logische Adresse besteht aus zwei Teilen:  Um diese Übersetzung zu beschleunigen, da jeder Speicherzugriff sonst einen zusätzlichen Zugriff auf die Seitentabelle im RAM bedeuten würde, gibt es einen speziellen Hardware-Cache, den  Virtueller Speicher und Seitenfehler Das Konzept des virtuellen Speichers erweitert Paging, indem es den Sekundärspeicher (Festplatte/SSD) als \"verlängerten Arm\" des Hauptspeichers nutzt. Ein Prozess kann einen sehr großen logischen Adressraum haben, von dem zu jedem Zeitpunkt nur die aktuell benötigten Seiten tatsächlich im physischen RAM liegen. Greift ein Prozess auf eine Adresse zu, deren Seite sich nicht im RAM befindet (in der Seitentabelle als \"not present\" markiert), löst die MMU einen  Dieses Verfahren, bei dem Seiten nur bei Bedarf geladen werden ( Seitenersetzungsalgorithmen Wenn bei einem Seitenfehler kein freier Rahmen verfügbar ist, muss das BS entscheiden, welche der bereits im Speicher befindlichen Seiten \"geopfert\" wird. Ersetze diejenige Seite, die in der Zukunft am längsten nicht mehr verwendet wird. Dieser Algorithmus ist nicht implementierbar, da er die Zukunft voraussehen müsste, dient aber als theoretischer Maßstab, um andere Algorithmen zu bewerten. Ersetze die Seite, die sich am längsten im Speicher befindet (die \"älteste\"). Einfach zu implementieren, aber oft ineffizient, da auch häufig genutzte Seiten verdrängt werden können, nur weil sie schon lange im Speicher sind. Ersetze die Seite, auf die am längsten nicht mehr zugegriffen wurde. Basiert direkt auf dem Lokalitätsprinzip und erzielt in der Praxis sehr gute Ergebnisse. Die Implementierung ist jedoch aufwändig, da das System bei jedem Speicherzugriff Zeitstempel oder eine geordnete Liste pflegen müsste. Ein effizienter und weit verbreiteter Algorithmus zur Annäherung an LRU. Die Seitenrahmen werden als zirkuläre Liste (\"Uhr\") verwaltet. Jede Seite hat ein  Thrashing Thrashing ist ein kritischer Systemzustand, bei dem das System fast seine gesamte Zeit mit dem Ein- und Auslagern von Seiten verbringt, anstatt nützliche Arbeit zu verrichten. Die CPU-Auslastung bricht ein, während die Festplattenaktivität extrem hoch ist.   Welche Strategie nutzt Seiten?  Erstellt zur Unterstützung der Prüfungsvorbereitung. Viel Erfolg bei der Klausur!"}, {"title": "7. Kommunikation - Lernportal Betriebssysteme", "href": "07_kommunikation.html", "text": "Kapitel 7: Kommunikation und Kooperation Wie Prozesse und Threads miteinander interagieren und Daten austauschen. Grundlagen der Prozessinteraktion Prozesse laufen selten isoliert ab. Für viele komplexe Aufgaben müssen sie zusammenarbeiten. Diese  Der Informationsaustausch selbst lässt sich in zwei grundlegende Modelle unterteilen: Das Betriebssystem muss für beide Modelle Mechanismen bereitstellen, die als  1. Kommunikation durch Nachrichtenaustausch Beim nachrichtenbasierten Ansatz stellen Betriebssysteme primitive Operationen zur Verfügung, typischerweise  Dies beschreibt, wie sich der aufrufende Prozess nach der Operation verhält. Eine Pipe ist ein gerichteter Kommunikationskanal, der einen Bytestrom nach dem  IPC-Dienste wie POSIX-Message-Queues oder System V Message Queues stellen Warteschlangen zur Verfügung: Sockets sind generelle Kommunikationsendpunkte, häufig für Netzwerkprogrammierung, aber auch lokal über „Unix Domain Sockets“: Signale sind eine einfache, asynchrone Form der Benachrichtigung, um einen Prozess über ein Ereignis zu informieren. Die \"Nachricht\" ist dabei nur eine einfache Nummer (der Signaltyp). Typische Signale sind  2. Kooperation durch gemeinsame Daten Bei diesem Modell arbeiten Prozesse nicht durch Nachrichtenaustausch, sondern durch den direkten Lese- und Schreibzugriff auf gemeinsame Datenobjekte. Dies ist zwar sehr schnell, erfordert aber eine explizite Synchronisation durch den Programmierer, um Race Conditions zu verhindern. Gemeinsamer Zugriff erfordert Synchronisation auf Objekte: Der effizienteste IPC-Mechanismus. Das Betriebssystem blendet (mapped) einen physischen Speicherbereich in die virtuellen  Der Code (aus  Alternative zu Shared Memory: Zugriff auf gemeinsame Dateien über  3. IPC-Dienste in Windows Wie kommunizieren Prozesse auf demselben Rechner am effizientesten?  Erstellt zur Unterstützung der Prüfungsvorbereitung. Viel Erfolg bei der Klausur!"}, {"title": "8. Deadlocks - Lernportal Betriebssysteme", "href": "08_deadlocks.html", "text": "Kapitel 8: Deadlocks (Verklemmungen) Der fatale Stillstand, wenn Prozesse ewig aufeinander warten. Was ist ein Deadlock? Da alle Prozesse warten, kann keiner von ihnen das Ereignis auslösen, auf das die anderen warten. Das Resultat ist ein permanenter Stillstand, den die Prozesse nicht von selbst auflösen können. Betroffene Ressourcen sind typischerweise  Fünf Philosophen sitzen am Tisch und tun zwei Dinge: denken oder essen. Zum Essen benötigt jeder Philosoph zwei Essstäbchen. Zwischen je zwei Philosophen liegt genau ein Stäbchen. Ein anschauliches Beispiel findet sich im Schienenverkehr oder bei Verkehrsstaus, wo sich Fahrzeuge gegenseitig blockieren. Ähnliche Situationen treten in Datenbanken auf, wenn Transaktionen sich gegenseitig sperren. Quelle:  Die vier notwendigen Bedingungen für einen Deadlock Ein Deadlock kann nur dann auftreten, wenn  Der Betriebsmittelgraph (Resource-Allocation Graph) Ein Betriebsmittelgraph ist ein gerichteter Graph, der den Zustand der Ressourcenvergabe im System visualisiert. Er besteht aus: Im obigen komplexeren Graph werden mehrere Ressourceninstanzen betrachtet. Ein Zyklus zeigt hier nicht zwingend einen Deadlock an, da Prozesse auf andere Instanzen derselben Ressource zugreifen können. Strategien zum Umgang mit Deadlocks Die einfachste Strategie ist, das Problem zu ignorieren. Man geht davon aus, dass Deadlocks so selten auftreten, dass der Aufwand für ihre Behandlung in keinem Verhältnis zum Nutzen steht. Wenn ein Deadlock doch einmal auftritt, wird das System manuell oder per Watchdog neu gestartet. Dies ist der gängige Ansatz in vielen Desktop-Betriebssystemen. Das System lässt Deadlocks zu, verfügt aber über Mechanismen, um sie zu erkennen und aufzulösen. Hierbei wird strukturell sichergestellt, dass das System niemals in einen Deadlock-Zustand geraten kann, indem mindestens eine der vier notwendigen Bedingungen von vornherein ausgeschlossen wird. Diese dynamische Methode erfordert, dass jeder Prozess vorab seinen  Das bekannteste Verfahren hierfür ist der  Welche Bedingung ist kein notwendiger Teil eines Deadlocks?  Erstellt zur Unterstützung der Prüfungsvorbereitung. Viel Erfolg bei der Klausur!"}, {"title": "4. Threads - Lernportal Betriebssysteme", "href": "04_threads.html", "text": "Kapitel 4: Threads Leichtgewichtige Prozesse für effiziente Parallelität. Was sind Threads? Ein Thread (engl. für \"Faden\" im Sinne eines Ausführungsstrangs) ist die kleinste Ausführungseinheit, die von einem  Das entscheidende Merkmal von Threads ist die Aufteilung der Ressourcen:   Threads im Vergleich zu  Warum Threads verwenden? (Vorteile) Implementierungsmodelle: User-Threads vs. Kernel-Threads Die gesamte Thread-Verwaltung (Erzeugung, Scheduling,  Jeder Thread wird direkt vom Betriebssystem-Kernel verwaltet. Der Kernel kennt jeden Thread und plant (scheduled) ihn wie einen Prozess. Dies ist der Ansatz, der von modernen Betriebssystemen wie Windows und Linux verwendet wird.  Threads in Java Java hat die Thread-Unterstützung tief in die Sprache und die Java Virtual Machine (JVM) integriert. Jedes Java-Programm läuft in mindestens einem Thread, dem sogenannten \"main-thread\". Es gibt zwei primäre Wege, einen Thread zu deklarieren und zu erzeugen: In beiden Fällen enthält die  Dieses Beispiel zeigt, wie zwei Threads erzeugt werden, die nebenläufig unterschiedliche Zeichen auf der Konsole ausgeben. Die Ausgabe ist nicht-deterministisch und wird bei jedem Lauf anders durchmischt sein. Dieses Beispiel demonstriert eine  Mögliche Ausgabe:  Weiterführende Links Was teilen sich Threads innerhalb eines Prozesses?  Erstellt zur Unterstützung der Prüfungsvorbereitung. Viel Erfolg bei der Klausur!"}, {"title": "Lernportal Betriebssysteme - Lernportal Betriebssysteme", "href": "index.html", "text": "Lernportal Betriebssysteme Ihre detaillierte und interaktive Vorbereitung auf die Klausur. Willkommen! Diese Webseite dient als zentrale Anlaufstelle für Ihre Klausurvorbereitung im Fach Betriebssysteme. Jedes Kapitel der Vorlesung wurde hier detailliert aufbereitet und mit Beispielen angereichert. Wählen Sie ein Thema aus der folgenden Übersicht, um zu den Lerninhalten zu gelangen. Themenübersicht Erstellt zur Unterstützung der Prüfungsvorbereitung. Viel Erfolg bei der Klausur! Erstellt zur Unterstützung der Prüfungsvorbereitung. Viel Erfolg bei der Klausur!"}, {"title": "Übungen - Lernportal Betriebssysteme", "href": "uebungen.html", "text": "Übungsaufgaben Teste dein Wissen mit einer kleinen Quizfrage. Welche Aufgabe hat ein Betriebssystem?  Erstellt zur Unterstützung der Prüfungsvorbereitung. Viel Erfolg bei der Klausur!"}]
